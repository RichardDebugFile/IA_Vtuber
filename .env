# ---- Otros servicios (tal cual los tienes) ----
GATEWAY_HTTP=http://127.0.0.1:8765
OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=gemma3

# ---- Fish Speech: rutas (MIGRADAS AL PROYECTO - ahora son relativas) ----
# fish-speech ahora está en services/tts/vendor/fish-speech
FISH_REPO=services/tts/vendor/fish-speech
FISH_VENV_PY=services/tts/vendor/fish-speech/.fs/Scripts/python.exe
FISH_CKPT=services/tts/models/fish-speech-1.5

# Endpoint HTTP de Fish (lo usa el TTS cliente)
FISH_HOST=127.0.0.1
FISH_PORT=8080
FISH_TTS_HTTP=http://127.0.0.1:8080/v1/tts

# Config del decoder de Fish Speech (firefly para fish-speech-1.5)
FISH_DECODER_CONFIG=firefly_gan_vq

# Tiempo máximo de arranque (segundos)
# Aumentado a 600s (10 min) para permitir compilación inicial con --compile
FISH_START_TIMEOUT=600

# ---- Límite de VRAM (optimizado para RTX 5060 Ti 16GB) ----
# OPTIMIZACIÓN CRÍTICA: Fish Speech 1.5 requiere 12GB para rendimiento óptimo
# RTX 5060 Ti tiene 16GB disponibles - usar 12GB (75% del total)
FISH_CUDA_MAX_GB=12.0
FISH_CUDA_VERBOSE=1

# ---- Torch Compile (10x speedup) ----
# DESHABILITADO: Requiere Triton que no tiene soporte oficial en Windows
# Para habilitar en Linux/WSL: FISH_ENABLE_COMPILE=1
# Alternativa Windows: Usar parámetros optimizados (ya implementado en engine_http.py)
FISH_ENABLE_COMPILE=0

# Asignador CUDA (con expandable_segments para evitar fragmentación)
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:32,garbage_collection_threshold:0.7

# Fijar la GPU a usar
CUDA_VISIBLE_DEVICES=0

# -------------------------------------------------------
# Voz fija (referencia local)
# -------------------------------------------------------
# Usando audio de 30s (mejor rendimiento que 15s según benchmarks)
FISH_REF_WAV=F:\Documentos F\GitHub\IA_Vtuber\services\tts\reference\casiopy\CasiopyVoz-muestraWAV.wav
FISH_REF_TXT=Oh, con que te quedó dudas sobre Mochi. Bien te concederé un desvío de nuestras tareas en mano, ya que lo pediste de forma tan... amable y tierna.
FISH_REF_ID=casiopy-full-30s
FISH_USE_MEMORY_CACHE=on

# === Assistant chunking / pacing (opcionales) ===
ASSISTANT_MAX_WORDS=9
ASSISTANT_TTS_BUDGET_MS=9000
TTS_SEC_PER_WORD=1.2
ASSISTANT_TTS_OVERHEAD_MS=700
ASSISTANT_DUR_MS_PER_WORD=480

# reduce troceo y evita oraciones partidas raro
ASSISTANT_LONG_SPLIT_WORDS=28
ASSISTANT_LONG_MIN_HALF=10
ASSISTANT_SHORT_MERGE_HEAD=4

# respira lo justo
ASSISTANT_GAP_MIN_S=0.08
ASSISTANT_WARMUP_MS=3000

# TTS en 3080 10GB
ASSISTANT_TTS_CONCURRENCY=2   # sube a 2 si ves que tu VRAM lo aguanta
ASSISTANT_PREBUFFER_CHUNKS=1

# fusión dinámica para evitar huecos largos
ASSISTANT_MERGE_LAG_MS=2500
ASSISTANT_MAX_CHUNKS=10

ASSISTANT_CHUNK_OUT=url
ASSISTANT_FIRST_MAX_WORDS=6
ASSISTANT_FIRST_MIN_WORDS=2
ASSISTANT_PAUSE_COMMA_MS=120
ASSISTANT_PAUSE_STOP_MS=220
ASSISTANT_PAUSE_DEFAULT_MS=80




