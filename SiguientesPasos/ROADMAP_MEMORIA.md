# Roadmap ‚Äî Sistema de Memoria y Evoluci√≥n de Casiopy

> **Objetivo:** Transformar a Casiopy de un LLM gen√©rico con voz a una IA que recuerda,
> aprende y evoluciona con el tiempo ‚Äî como Neuro-sama pero con arquitectura propia.

**Creado:** 27 de febrero de 2026
**√öltima revisi√≥n:** 27 de febrero de 2026
**Estado general:** üü° En planificaci√≥n

---

## Diagn√≥stico de partida

El sistema est√° **85% construido pero desconectado**. Todos los servicios existen
pero no hablan entre s√≠:

```
HOY:
User ‚Üí conversation/ ‚Üí Ollama (modelo base gen√©rico, sin personalidad)

OBJETIVO:
User ‚Üí conversation/ ‚Üí memory-service/ ‚Üí Core Memory + LoRA Pers. + LoRA Epis√≥dico ‚Üí Ollama (casiopy:latest)
           ‚Üë almacena cada turno              ‚Üë inyecta contexto y recuerdos
```

### Gaps cr√≠ticos identificados

| # | Gap | Impacto |
|---|-----|---------|
| G1 | `conversation/` no conectado a `memory-service/` | Sin almacenamiento, sin evoluci√≥n |
| G2 | LoRA de personalidad entrenado pero NO deployado en Ollama | Casiopy responde como LLM gen√©rico |
| G3 | Sin historial multi-turno persistente | Olvida dentro de la misma sesi√≥n |
| G4 | Sin entrenamiento epis√≥dico autom√°tico semanal | Capa 2 (aprendizaje) nunca se activa |
| G5 | Sin b√∫squeda sem√°ntica RAG (pgvector) | No puede recuperar recuerdos espec√≠ficos |
| G6 | Sin validaci√≥n anti-lobotom√≠a integrada | Riesgo de deployar un LoRA que olvida la personalidad |

---

## Fase 1 ‚Äî Conectar lo que ya existe
**Per√≠odo estimado: 28 feb ‚Äì 7 mar 2026**
**Estado: üî¥ Pendiente**

Objetivo: Hacer que Casiopy use su personalidad fine-tuneada y guarde cada conversaci√≥n.

---

### Tarea 1.1 ‚Äî Deployar LoRA de personalidad en Ollama
**Fecha objetivo: 28 feb 2026**
**Duraci√≥n estimada: 2-4 horas**
**Responsable: Richard**

El checkpoint `personality_v2_refined_20251230_163256` ya existe y est√° entrenado.
Solo hay que ejecutar el script de deploy.

**Pasos:**
```bash
cd services/memory-service
# Activar entorno de entrenamiento
scripts/setup/activate_training_env.bat

# Ejecutar deploy (fusiona LoRA + convierte a GGUF + registra en Ollama)
python scripts/deploy_to_ollama.py --version v2
```

**Resultado esperado:**
- Ollama tendr√° un modelo `casiopy:v2` disponible
- `ollama list` mostrar√° el nuevo modelo

**Criterio de √©xito:**
```bash
ollama run casiopy:v2 "¬øC√≥mo te llamas y qu√© sabes hacer?"
# Debe responder con la personalidad de Casiopy, no como LLM gen√©rico
```

**Notas:**
- Si el deploy_to_ollama.py falla, revisar si necesita ajustes de rutas (las rutas
  en el script pueden ser relativas y necesitar ajuste)
- Alternativa manual: usar `ollama create` con un Modelfile que apunte al GGUF generado

---

### Tarea 1.2 ‚Äî Conectar conversation/ con memory-service/
**Fecha objetivo: 1-3 mar 2026**
**Duraci√≥n estimada: 1-2 d√≠as**
**Archivos a modificar:** `services/conversation/src/server.py`

Actualmente `server.py` usa un system prompt hardcodeado de una l√≠nea y no guarda nada.
Hay que a√±adir 5 llamadas HTTP al memory-service para cerrar el ciclo.

**Flujo objetivo:**
```
/chat recibido
    ‚Üì
1. POST /sessions ‚Üí crear sesi√≥n (memory-service)
    ‚Üì
2. GET /core-memory/system-prompt/generate ‚Üí obtener system prompt con personalidad
    ‚Üì
3. Llamar a Ollama con system prompt real + historial de sesi√≥n
    ‚Üì
4. POST /interactions ‚Üí almacenar par input/output con calidad autom√°tica
    ‚Üì
5. POST /sessions/{id}/end ‚Üí cerrar sesi√≥n
```

**Puntos de atenci√≥n:**
- El memory-service debe estar corriendo (puerto 8820) y PostgreSQL activo (Docker)
- El quality score se calcula autom√°ticamente en `interaction_manager.py`
- Manejar el caso de que memory-service est√© ca√≠do (fallback: responder igual sin guardar)

**Criterio de √©xito:**
- Despu√©s de una conversaci√≥n, `GET http://localhost:8820/interactions/recent` muestra los turnos
- El system prompt que llega a Ollama contiene la identidad y personalidad de Casiopy

---

### Tarea 1.3 ‚Äî Historial multi-turno dentro de una sesi√≥n
**Fecha objetivo: 3-5 mar 2026**
**Duraci√≥n estimada: 1 d√≠a**
**Archivos a modificar:** `services/conversation/src/server.py`

Actualmente cada `/chat` es un request independiente ‚Äî Casiopy olvida lo que dijo hace
2 mensajes. Hay que mantener el hilo de conversaci√≥n mientras la sesi√≥n est√° activa.

**Dise√±o:**
- Usar un dict en memoria `active_sessions: dict[session_id, list[messages]]`
- Cada turno: append del user message + assistant reply
- Mantener ventana de los √∫ltimos 20 turnos (evitar context overflow)
- Limpiar sesi√≥n cuando se llame a finalizar o tras N minutos de inactividad

**Criterio de √©xito:**
```
User: "Me llamo Richard"
Casiopy: "Ah, hola Richard..."
User: "¬øC√≥mo me llamo?"
Casiopy: "Te llamas Richard, lo dijiste hace un momento."  ‚Üê esto debe funcionar
```

---

### Tarea 1.4 ‚Äî Actualizar OLLAMA_MODEL en conversation/
**Fecha objetivo: 5 mar 2026**
**Duraci√≥n estimada: 15 minutos**
**Archivos a modificar:** `services/conversation/.env`

Una vez el deploy de 1.1 est√© listo, cambiar el modelo:
```bash
# services/conversation/.env
OLLAMA_MODEL=casiopy:v2   # antes era: gemma3
```

**Criterio de √©xito:** Conversaci√≥n entera con Casiopy usando su personalidad real.

---

### Hito 1 ‚Äî Verificaci√≥n completa de Fase 1
**Fecha objetivo: 7 mar 2026**

Prueba de integraci√≥n completa:
- [ ] `ollama list` muestra `casiopy:v2`
- [ ] Conversaci√≥n de 10 turnos ‚Äî Casiopy mantiene contexto
- [ ] `GET /interactions/recent` muestra los 10 turnos guardados con embeddings
- [ ] System prompt de Ollama contiene Core Memory (verificar con logs)
- [ ] Casiopy responde con su personalidad (sarcasmo, referencias a su historia)

---

## Fase 2 ‚Äî El bucle de aprendizaje autom√°tico
**Per√≠odo estimado: 8 mar ‚Äì 22 mar 2026**
**Estado: üî¥ Pendiente**

Objetivo: Que Casiopy aprenda autom√°ticamente de las conversaciones cada semana.

---

### Tarea 2.1 ‚Äî Quality scoring y panel de feedback
**Fecha objetivo: 8-10 mar 2026**
**Duraci√≥n estimada: 2 d√≠as**

El `interaction_manager.py` ya calcula un quality score autom√°tico, pero hay que
a√±adir feedback manual del creador (t√∫) para supervisar qu√© entra al entrenamiento.

**D√≥nde a√±adirlo:**
- Panel en el monitoring dashboard (puerto 8900) ‚Äî nueva p√°gina `memory.html`
- Listado de interacciones recientes con botones: ‚úÖ Buena respuesta | ‚ùå Mala | ‚úèÔ∏è Correcci√≥n
- La correcci√≥n permite escribir c√≥mo deber√≠a haber respondido Casiopy

**Endpoints del memory-service ya disponibles:**
- `POST /feedback` ‚Äî a√±ade retroalimentaci√≥n
- `PUT /interactions/{id}/quality` ‚Äî ajusta el score manualmente

---

### Tarea 2.2 ‚Äî Pipeline de exportaci√≥n autom√°tico
**Fecha objetivo: 10-12 mar 2026**
**Duraci√≥n estimada: 2 d√≠as**
**Archivos a modificar:** `services/memory-service/src/main.py`

A√±adir un scheduler (APScheduler) que ejecute el pipeline de exportaci√≥n cada domingo.

**Flujo autom√°tico semanal:**
```
Domingo 23:00
    ‚Üì
1. export_training_data.py ‚Äî exporta interacciones quality >= 0.6 de los √∫ltimos 7 d√≠as
    ‚Üì
2. validate_dataset.py ‚Äî verifica que el dataset es v√°lido y tiene suficientes ejemplos (m√≠n. 50)
    ‚Üì
3. Si OK: train_episodic_lora.py ‚Äî entrena LoRA Capa 2 (15-30 min, ~6GB VRAM)
    ‚Üì
4. test_personality.py ‚Äî validaci√≥n anti-lobotom√≠a (¬øsigue siendo Casiopy?)
    ‚Üì
5. Si OK: deploy_to_ollama.py --week N ‚Äî fusiona Capa 1 + Capa 2 ‚Üí Ollama casiopy:weekN
    ‚Üì
6. Si falla: revertir a semana anterior + notificar en logs
```

**Criterio de √©xito:**
- Despu√©s de la primera semana de uso, aparece `casiopy:week1` en Ollama
- Las respuestas reflejan temas de los que se habl√≥ esa semana

---

### Tarea 2.3 ‚Äî Notificaciones de entrenamiento en monitoring
**Fecha objetivo: 12-14 mar 2026**
**Duraci√≥n estimada: 1 d√≠a**

Mostrar en el monitoring dashboard:
- Estado del √∫ltimo entrenamiento epis√≥dico (fecha, loss, √©xito/fallo)
- Pr√≥ximo entrenamiento programado
- N√∫mero de interacciones acumuladas esta semana (con indicador de si son suficientes)
- Historial de versiones de LoRA deployadas

---

### Hito 2 ‚Äî Primera semana de aprendizaje real
**Fecha objetivo: 22 mar 2026**

- [ ] Primera semana de conversaciones acumuladas en PostgreSQL
- [ ] Pipeline autom√°tico ejecut√≥ exitosamente el domingo
- [ ] `casiopy:week1` disponible en Ollama
- [ ] Respuestas de week1 reflejan los temas de esa semana
- [ ] Panel de feedback operativo con al menos 20 interacciones evaluadas

---

## Fase 3 ‚Äî Memoria epis√≥dica real (largo plazo)
**Per√≠odo estimado: 23 mar ‚Äì 15 abr 2026**
**Estado: üî¥ Pendiente**

Objetivo: Que Casiopy pueda recuperar recuerdos espec√≠ficos de conversaciones pasadas
en tiempo real ‚Äî "la semana pasada cuando hablamos de Oshi no Ko..."

---

### Tarea 3.1 ‚Äî B√∫squeda sem√°ntica con pgvector
**Fecha objetivo: 23-27 mar 2026**
**Duraci√≥n estimada: 3-4 d√≠as**

La infraestructura de pgvector ya est√° en la BD (`input_embedding` y `output_embedding`
son columnas vector de 384 dims). Hay que usarla.

**Flujo de recuperaci√≥n de memoria:**
```python
# En conversation/src/server.py, antes de llamar a Ollama:

# 1. Generar embedding del mensaje del usuario
user_embedding = embed(user_text)  # llamada a memory-service/embed

# 2. Buscar interacciones pasadas similares
memories = GET /memory-service/search?embedding=...&threshold=0.75&limit=3

# 3. Inyectar como contexto adicional al system prompt
context = format_memories(memories)
system_prompt = base_system_prompt + "\n\n[RECUERDOS RELEVANTES]\n" + context
```

**Criterio de √©xito:**
```
User: "¬øQu√© piensas de Oshi no Ko?"
Casiopy: "Oye, creo que ya hablamos de esto... [referencia a conversaci√≥n anterior]"
```

---

### Tarea 3.2 ‚Äî A√±adir endpoint de b√∫squeda sem√°ntica al memory-service
**Fecha objetivo: 23-25 mar 2026**
**Duraci√≥n estimada: 1-2 d√≠as**
**Archivos a modificar:** `services/memory-service/src/main.py`

```python
@app.get("/search")
async def semantic_search(query: str, threshold: float = 0.75, limit: int = 5):
    embedding = embedding_service.encode(query)
    results = await db.fetch("""
        SELECT input_text, output_text, created_at,
               1 - (input_embedding <-> $1::vector) AS similarity
        FROM interactions
        WHERE 1 - (input_embedding <-> $1::vector) > $2
        ORDER BY similarity DESC
        LIMIT $3
    """, embedding, threshold, limit)
    return results
```

---

### Tarea 3.3 ‚Äî Personalidad drifting controlado
**Fecha objetivo: 1-10 abr 2026**
**Duraci√≥n estimada: 1 semana**

Implementar un sistema que analice qu√© rasgos de personalidad refuerza la audiencia:
- Qu√© emociones genera m√°s interacciones positivas
- Qu√© temas generan m√°s engagement
- Ajustar los pesos de muestreo en el dataset semanal para amplificar esos rasgos

**Objetivo:** Casiopy "evoluciona" en la direcci√≥n que su audiencia moldea, sin perder
su identidad core (Capa 0 sigue siendo inmutable).

---

### Hito 3 ‚Äî Memoria epis√≥dica operativa
**Fecha objetivo: 15 abr 2026**

- [ ] B√∫squeda sem√°ntica funcional en < 200ms
- [ ] Casiopy menciona conversaciones pasadas de forma natural
- [ ] Al menos 4 semanas de LoRAs epis√≥dicos acumulados
- [ ] M√©tricas de drift de personalidad visibles en el dashboard
- [ ] Casiopy es notablemente diferente a un LLM gen√©rico en conversaci√≥n libre

---

## Resumen visual del calendario

```
Feb 2026
‚îî‚îÄ‚îÄ 28 feb ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [1.1] Deploy LoRA personalidad ‚Üí Ollama casiopy:v2

Mar 2026
‚îú‚îÄ‚îÄ 01-03 mar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [1.2] Conectar conversation ‚Üî memory-service
‚îú‚îÄ‚îÄ 03-05 mar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [1.3] Historial multi-turno
‚îú‚îÄ‚îÄ 05 mar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [1.4] Cambiar OLLAMA_MODEL a casiopy:v2
‚îú‚îÄ‚îÄ 07 mar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚úÖ HITO 1: Casiopy tiene memoria b√°sica
‚îÇ
‚îú‚îÄ‚îÄ 08-10 mar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [2.1] Panel de feedback en monitoring
‚îú‚îÄ‚îÄ 10-12 mar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [2.2] Pipeline de exportaci√≥n + entrenamiento autom√°tico
‚îú‚îÄ‚îÄ 12-14 mar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [2.3] Notificaciones de entrenamiento en dashboard
‚îú‚îÄ‚îÄ 22 mar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚úÖ HITO 2: Primera semana de aprendizaje real

Abr 2026
‚îú‚îÄ‚îÄ 23-27 mar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [3.1] B√∫squeda sem√°ntica pgvector
‚îú‚îÄ‚îÄ 23-25 mar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [3.2] Endpoint /search en memory-service
‚îú‚îÄ‚îÄ 01-10 abr ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [3.3] Personalidad drifting
‚îî‚îÄ‚îÄ 15 abr ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚úÖ HITO 3: Memoria epis√≥dica operativa
```

---

## Dependencias entre tareas

```
1.1 (Deploy LoRA)
 ‚îî‚Üí 1.4 (Cambiar OLLAMA_MODEL)

1.2 (Conectar conversation ‚Üî memory)
 ‚îî‚Üí 1.3 (Historial multi-turno)
     ‚îî‚Üí 2.1 (Panel feedback)
         ‚îî‚Üí 2.2 (Pipeline autom√°tico)
             ‚îî‚Üí 2.3 (Notificaciones dashboard)
                 ‚îî‚Üí 3.1 (B√∫squeda sem√°ntica)
                     ‚îî‚Üí 3.2 (Endpoint /search)
                         ‚îî‚Üí 3.3 (Personality drifting)
```

**Bloqueo cr√≠tico:** La Tarea 1.2 es la m√°s importante. Sin ella,
nada de Fase 2 ni Fase 3 es posible.

---

## Decisiones pendientes

| # | Decisi√≥n | Opciones | Impacto |
|---|----------|---------|---------|
| D1 | ¬øTama√±o de ventana de historial por sesi√≥n? | 10 / 20 / 50 turnos | VRAM de Ollama |
| D2 | ¬øUmbral m√≠nimo de ejemplos para entrenar Capa 2? | 30 / 50 / 100 | Calidad del LoRA epis√≥dico |
| D3 | ¬øFrecuencia del entrenamiento epis√≥dico? | Diario / Semanal / Mensual | VRAM y tiempo de c√≥mputo |
| D4 | ¬øEl drift de personalidad es autom√°tico o supervisado? | Auto / Manual / H√≠brido | Riesgo de desviaci√≥n indeseada |
| D5 | ¬øChromaDB adem√°s de pgvector, o solo pgvector? | Ambos / Solo pgvector | Complejidad vs. capacidad |

---

## Notas t√©cnicas relevantes

- **VRAM disponible:** RTX 5060 Ti (16 GB) ‚Äî suficiente para LoRA Capa 1 (~8GB) y Capa 2 (~6GB)
- **Base model:** `NousResearch/Hermes-3-Llama-3.1-8B` con 4-bit quantization
- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2` (384 dims, ya en memory-service)
- **PostgreSQL:** Docker, puerto 8821, con extensi√≥n pgvector ya configurada
- **LoRA actual listo:** `personality_v2_refined_20251230_163256` ‚Äî Loss: 0.033, ~9.2 epochs efectivos

---

*Documento generado el 27 de febrero de 2026. Actualizar fechas y estados conforme avance el proyecto.*
