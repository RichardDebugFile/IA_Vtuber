# Conversation Service ‚Äî Casiopy

Servicio FastAPI que orquesta el flujo de conversaci√≥n de Casiopy: recibe mensajes del usuario,
enriquece el contexto con memoria sem√°ntica, llama al LLM (Ollama) y registra la interacci√≥n.

## Puerto

`8820` (configurado en `.env`)

## Variables de entorno

| Variable | Default | Descripci√≥n |
|----------|---------|-------------|
| `OLLAMA_HOST` | `http://127.0.0.1:11434` | URL de Ollama |
| `OLLAMA_MODEL` | `gemma3` | Modelo activo (p.ej. `casiopy:week05`) |
| `MEMORY_HTTP` | `http://127.0.0.1:8820` | URL del memory-service |
| `GATEWAY_HTTP` | `http://127.0.0.1:8800` | URL del gateway (no usado actualmente) |

## Iniciar

```bash
cd services/conversation
python -m uvicorn src.server:app --host 0.0.0.0 --port 8820 --reload
```

---

## Arquitectura del flujo de chat

```
POST /chat
    ‚îÇ
    ‚îú‚îÄ 1. Crear/recuperar sesi√≥n en memory-service
    ‚îÇ       POST /sessions  (si usuario nuevo)
    ‚îÇ
    ‚îú‚îÄ 2. Obtener system prompt de Core Memory
    ‚îÇ       GET /core-memory/system-prompt  (cach√© 5 min)
    ‚îÇ
    ‚îú‚îÄ 3. B√∫squeda sem√°ntica de recuerdos relacionados  ‚Üê Fase 3
    ‚îÇ       GET /search?q={texto}&threshold=0.75&limit=3&days=90
    ‚îÇ       (timeout 3s ‚Äî degradaci√≥n graceful si falla)
    ‚îÇ
    ‚îú‚îÄ 4. Construir mensajes al LLM
    ‚îÇ       system = core_memory_prompt + [RECUERDOS RELACIONADOS]
    ‚îÇ       messages = [system] + historial + [user message]
    ‚îÇ
    ‚îú‚îÄ 5. Llamar a Ollama
    ‚îÇ       POST http://ollama/api/chat
    ‚îÇ
    ‚îú‚îÄ 6. Clasificar emoci√≥n de la respuesta
    ‚îÇ       src/emotion.py  (heur√≠stico basado en palabras clave)
    ‚îÇ
    ‚îú‚îÄ 7. Registrar interacci√≥n en memory-service
    ‚îÇ       POST /interactions  (session_id, input, output, emociones)
    ‚îÇ       El embedding se genera en background (no bloquea la respuesta)
    ‚îÇ
    ‚îî‚îÄ 8. Actualizar historial en memoria local
            M√°ximo 20 turnos por sesi√≥n (_MAX_HISTORY_TURNS)
```

---

## Endpoints

### `POST /chat`

Env√≠a un mensaje y recibe la respuesta de Casiopy.

**Body:**
```json
{
  "user_id": "stream_viewer_42",
  "text": "¬øQu√© juego est√°s jugando hoy?"
}
```

**Response:**
```json
{
  "response": "Hoy toca Dark Souls 3, como siempre tortur√°ndome üê≤",
  "emotion": "playful",
  "session_id": "sess_abc123",
  "turn": 3,
  "memories_used": 2
}
```

| Campo | Descripci√≥n |
|-------|-------------|
| `response` | Respuesta generada por el LLM |
| `emotion` | Emoci√≥n clasificada de la respuesta |
| `session_id` | ID de sesi√≥n en el memory-service |
| `turn` | N√∫mero de turno en la sesi√≥n actual |
| `memories_used` | N√∫mero de recuerdos sem√°nticos inyectados |

### `GET /health`

Estado del servicio y conectividad con Ollama.

### `GET /models`

Lista de modelos disponibles en Ollama.

### `DELETE /session/{user_id}`

Resetea la sesi√≥n activa de un usuario (limpia historial en memoria local).

---

## Memoria sem√°ntica (Fase 3)

Antes de cada llamada al LLM, el servicio recupera hasta **3 interacciones pasadas**
sem√°nticamente similares al mensaje actual y las a√±ade al system prompt:

```
[RECUERDOS RELACIONADOS CON ESTE TEMA]
- (82% similar) Usuario: "¬øCu√°l es tu juego favorito?" ‚Üí Casiopy: "Dark Souls, sin duda."
- (77% similar) Usuario: "¬øJuegas RPGs?" ‚Üí Casiopy: "Los JRPGs son mi g√©nero favorito..."
[FIN DE RECUERDOS]
```

**Configuraci√≥n:**
- Umbral de similitud: 0.75 (coseno)
- L√≠mite: 3 recuerdos por turno
- Ventana temporal: 90 d√≠as
- Timeout: 3 segundos (si falla, contin√∫a sin recuerdos)

El embedding de cada interacci√≥n se genera autom√°ticamente en background al registrarla
(via `POST /interactions` en el memory-service).

---

## Sesiones y historial

- Las sesiones se crean autom√°ticamente al primer mensaje de cada `user_id`
- El historial (m√°x. 20 turnos) se mantiene en memoria local (`_active_sessions`)
- El historial persiste en el memory-service v√≠a `POST /interactions`
- Al reiniciar el servicio, las sesiones en memoria se pierden pero las interacciones
  permanecen en PostgreSQL

---

## M√≥dulos

| Archivo | Descripci√≥n |
|---------|-------------|
| `server.py` | FastAPI app ‚Äî endpoints, sesiones, inyecci√≥n de memoria |
| `llm_ollama.py` | Cliente HTTP para Ollama (`/api/chat`) |
| `emotion.py` | Clasificaci√≥n de emoci√≥n heur√≠stica |
| `ollama_manager.py` | Gesti√≥n del proceso Ollama (start/stop/health) |
| `tools_registry.py` | Registro de herramientas/tools para el LLM |

---

## Degradaci√≥n graceful

El servicio funciona aunque el memory-service no est√© disponible:
- Sin memory-service ‚Üí sistema prompt por defecto (`_DEFAULT_SYSTEM_PROMPT`)
- Sin b√∫squeda sem√°ntica ‚Üí responde sin contexto de recuerdos
- Sin registro de interacciones ‚Üí conversaci√≥n no se guarda, pero el usuario recibe respuesta

Los errores se loguean pero no interrumpen el flujo.

---

**Versi√≥n**: 1.2.0 (Fase 3: inyecci√≥n de memoria sem√°ntica)
**√öltima actualizaci√≥n**: 2026-02-28
