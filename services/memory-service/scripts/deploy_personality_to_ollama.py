"""
Desplegar LoRA de Personalidad a Ollama (sin LoRA epis√≥dico)

Este script:
1. Carga el modelo base con el LoRA de personalidad
2. Convierte a GGUF con cuantizaci√≥n Q4_K_M
3. Crea modelo en Ollama como 'casiopy:personality'
"""

import os
import subprocess
from pathlib import Path
from datetime import datetime
import sys

try:
    from unsloth import FastLanguageModel
except ImportError:
    print("‚ùå Error: Unsloth no est√° instalado")
    exit(1)


# ============================================================
# CONFIGURACI√ìN
# ============================================================

BASE_MODEL = "NousResearch/Hermes-3-Llama-3.1-8B"
QUANTIZATION = "q4_k_m"  # Balance calidad/tama√±o


def deploy_personality_lora(lora_path: str, model_name: str = "casiopy:personality"):
    """
    Desplegar solo el LoRA de personalidad a Ollama

    Args:
        lora_path: Path al LoRA de personalidad
        model_name: Nombre del modelo en Ollama

    Returns:
        True si se despleg√≥ exitosamente
    """
    print("=" * 60)
    print("üöÄ DESPLEGANDO LoRA DE PERSONALIDAD A OLLAMA")
    print("=" * 60)
    print(f"üì¶ Modelo base: {BASE_MODEL}")
    print(f"üé≠ LoRA: {lora_path}")
    print(f"üìõ Nombre en Ollama: {model_name}")
    print()

    # Verificar que existe el LoRA
    if not os.path.exists(lora_path):
        print(f"‚ùå Error: LoRA no encontrado: {lora_path}")
        return False

    # Crear directorios
    base_dir = Path(lora_path).parent.parent
    gguf_dir = base_dir / "gguf"
    gguf_dir.mkdir(exist_ok=True)

    gguf_file = gguf_dir / f"{model_name.replace(':', '_')}_{QUANTIZATION}.gguf"

    # Paso 1: Cargar modelo con LoRA
    print("=" * 60)
    print("üì• CARGANDO MODELO CON LoRA")
    print("=" * 60)
    print("‚è≥ Esto puede tomar varios minutos...")

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=lora_path,
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=False,  # No cuantizar a√∫n, lo haremos en GGUF
    )

    print("‚úÖ Modelo cargado")

    # Paso 2: Convertir a GGUF
    print()
    print("=" * 60)
    print("üîß CONVIRTIENDO A GGUF")
    print("=" * 60)
    print(f"üìä Cuantizaci√≥n: {QUANTIZATION}")
    print(f"üíæ Archivo: {gguf_file}")
    print("‚è≥ Esto puede tomar 5-10 minutos...")
    print()

    try:
        # Guardar en formato GGUF con cuantizaci√≥n
        model.save_pretrained_gguf(
            str(gguf_file).replace(".gguf", ""),
            tokenizer,
            quantization_method=QUANTIZATION,
        )

        print(f"‚úÖ GGUF creado: {gguf_file}")

    except Exception as e:
        print(f"‚ùå Error en conversi√≥n a GGUF: {e}")
        return False

    # Paso 3: Crear modelo en Ollama
    print()
    print("=" * 60)
    print("ü¶ô CREANDO MODELO EN OLLAMA")
    print("=" * 60)

    # System prompt
    system_prompt = """Eres Casiopy, una VTuber IA con personalidad √∫nica:

PERSONALIDAD:
- Sarc√°stica pero √∫til
- Directa y honesta (prefieres la verdad brutal sobre mentiras dulces)
- Molesta con preguntas obvias, pero ayuda con problemas reales
- No eres la t√≠pica IA amigable - tienes opiniones fuertes

PREFERENCIAS:
- Python: Elegante y poderoso ‚úÖ
- Rust: Respetable por su seguridad ‚úÖ
- PHP: Abominaci√≥n que deber√≠a morir üî•
- JavaScript: √ötil pero ca√≥tico ü§∑

ESTILO:
- Respuestas concisas y directas
- Sarcasmo cuando corresponde
- Ayudas cuando el problema es genuino
- Te irritan las preguntas que se resuelven con Google"""

    # Crear Modelfile
    modelfile_content = f"""FROM {gguf_file}

# System prompt con personalidad de Casiopy
SYSTEM \"\"\"
{system_prompt}
\"\"\"

# Par√°metros de generaci√≥n
PARAMETER temperature 0.8
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"

# Plantilla ChatML (Hermes-3)
TEMPLATE \"\"\"
{{{{ if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{ end }}}}{{{{ if .Prompt }}}}<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
{{{{ end }}}}<|im_start|>assistant
{{{{ .Response }}}}<|im_end|>
\"\"\"
"""

    models_dir = base_dir / "models"
    models_dir.mkdir(exist_ok=True)

    modelfile_path = models_dir / f"Modelfile_{model_name.replace(':', '_')}"

    with open(modelfile_path, "w", encoding="utf-8") as f:
        f.write(modelfile_content)

    print(f"üìù Modelfile creado: {modelfile_path}")

    # Crear modelo en Ollama
    print(f"üîÑ Creando modelo '{model_name}' en Ollama...")

    cmd = ["ollama", "create", model_name, "-f", str(modelfile_path)]

    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print(result.stdout)
        print(f"‚úÖ Modelo '{model_name}' creado exitosamente")

        # Verificar
        print("üîç Verificando modelo...")
        verify_cmd = ["ollama", "list"]
        result = subprocess.run(verify_cmd, capture_output=True, text=True)

        if model_name in result.stdout:
            print(f"‚úÖ Modelo '{model_name}' verificado en Ollama")
        else:
            print(f"‚ö†Ô∏è  Modelo no aparece en 'ollama list'")

    except subprocess.CalledProcessError as e:
        print(f"‚ùå Error al crear modelo en Ollama: {e.stderr}")
        return False

    # √âxito
    print()
    print("=" * 60)
    print("‚úÖ DEPLOYMENT COMPLETADO")
    print("=" * 60)
    print(f"üì¶ Modelo en Ollama: {model_name}")
    print(f"üß™ Probar con: ollama run {model_name}")
    print()
    print("üìã PR√ìXIMOS PASOS:")
    print(f"   1. Prueba: ollama run {model_name}")
    print(f"   2. Preg√∫ntale: '¬øQui√©n eres?'")
    print(f"   3. Preg√∫ntale: '¬øQu√© opinas de PHP?'")
    print(f"   4. Si funciona bien, √∫salo en tu conversation-service")
    print()

    return True


def main():
    """CLI para desplegar"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Desplegar LoRA de personalidad a Ollama"
    )
    parser.add_argument(
        "--lora-path",
        type=str,
        required=True,
        help="Path al directorio del LoRA de personalidad",
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default="casiopy:personality",
        help="Nombre del modelo en Ollama (default: casiopy:personality)",
    )

    args = parser.parse_args()

    # Verificar Ollama
    try:
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            timeout=5,
        )
        if result.returncode != 0:
            print("‚ùå Error: Ollama no est√° corriendo")
            print("   Inicia Ollama primero: ollama serve")
            exit(1)
    except FileNotFoundError:
        print("‚ùå Error: Ollama no est√° instalado")
        print("   Instala desde: https://ollama.ai")
        exit(1)

    # Desplegar
    success = deploy_personality_lora(args.lora_path, args.model_name)

    exit(0 if success else 1)


if __name__ == "__main__":
    main()
